COURSE 04 of 05:FOUNDATIONS OF CONVOLUTIONAL NEURAL NETWORKS

INDEX:
== WEEK 1 - CONVOLUTIONAL NEURAL NETWORKS ==
== WEEK 2 - DEEP CONVOLUTIONAL MODELS: CASE STUDIES ==
== WEEK 3 - OBJECT DETECTION ==
== WEEK 4 - SPECIAL APPLICATIONS: FACE RECOGNITION & NEURAL STYLE TRANSFER ==

== WEEK 1 - CONVOLUTIONAL NEURAL NETWORKS ==
> Convolutions
- a convolution involves applying a "filter" (or, kernel) to the input data
- we are "convolving" the input data with a filter
- say we had a 6 x 6 input image, and a 3 x 3 filter (1,1,1;0,0,0;-1,-1,-1), we:
* mutiply the filter by a matched segment of the image, get the sum product, report a single output
* for a 6 x 6 image, with a 3 x 3 filter, this produces a 4 x 4 "image" output
* this would appear as conv2d in keras / tf

> Edge detection:
- positive to negative edges refers to light to dark or dark to light edges
- different filters (or, kernels) allow for detection of diffent types of edges (say, horizontal or vertical)
- we can let the NN learn the filter parameters, so that the edge detection can be performed in a way that suits the data being read

> Padding & Applying filters:
- a (n x n ) image convolved with an (f x f) filter produces a ((n - f + 1) x (n - f + 1)) output image
- applying fliters can lead to 
* shrinking output (ie: smaller output image)
* information at corners / extremes of the image are used less often than those towards the center, so information is being lost
- Padding allows us to add a border of zeros around the image, which can help preserve the output size after convolution, and retain all information at the extremes
* so: 
(n x n ) image, with padding (p), convolved with an (f x f) filter produces a ((n + 2p - f + 1) x (n + 2p - f + 1)) output image
- padding is referred to as:
"valid" means "no padding".  
"same" results in padding the input such that the output has the same length as the original input. 

> Stride:
- affects the shift of the filter across the image, ie: stride of 2 will shift the filter by two columns and rows 
* so: 
(n x n ) image, with padding (p), and stride (s), convolved with an (f x f) filter produces a ((n + 2p - f) / s + 1) x ((n + 2p - f) / s + 1) output image
- if there's a partial overlap of the filter with the imgage, then an output value will not be produced

> Multi-channel image convolution:
- 3D filters (H x W x Channel / Depth) is also available for image convolutions
- the number of channels in the filter (depth) must match the number of channels in the image
- as earlier, a (6 x 6 x 3) image convolved with a (3 x 3 x 3) filter will produced a (4 x 4) output, assuming valid padding and stride of 1
* so: 
(n x n x nc) image, with padding (p), and stride (s), convolved with an (f x f x nc) filter produces a ((n + 2p - f) / s + 1) x ((n + 2p - f) / s + 1) output image, where nc = number of channels
- we can apply different filters (kernels defined differently) of same shape to the input image, then stack the outputs!!
* ie: for example, we can have a filter for vertical edge detection, horizontal edge detction, and other types of edge detction...awesome!

> A Layer of a ConvNet:
1. Apply filter to the image
2. Add bias (b) to each cell of the output
3. Apply non-linear activation function (ie: ReLU) to the output
4. Repeat for each filter used, then stack up the outputs to form a Layer of a ConvNet

- The values of the filters become the Weights applied to the input image that the DL NN will learn, as well as the Bias

- Notation:
f[l] = filter size in layer l
p[l] = padidng in layer l
s[l] = stride in layer l
nc[l] = number of filters
input: n[l-1]H x n[l-1]W x nc[l-1] = the height, width and # channels output from the previously layer, becoming input in the current layer, l-1
output: n[l]H x n[l]W x nc[l], where nc[l] is given the number of filters in the current layer
thus (for height & width):
n[l]H = (n[l-1]H + 2p[l]- f[l]) / s[l] + 1
each filter is:
f[l] x f[l] x nc[l-1]
activations:
a[l] = n[l]H x n[l]W x nc[l]
A[l] = m x n[l]H x n[l]W x nc[l], where m reflects # of examples, such as during batch gradient descent
weights:
f[l] x f[l] x nc[l-1] x nc[l], where nc[l] is # filters in current layer
bias:
nc[l], a single bias parameter for each filter in the current layer

> Pooling layer:
- max pooling takes the max of values within a filter; average pooling, the average within a filter
- parameters include:
* filter size (ie: 2 x 2, 3 x 3)
* stride (ie: slide across 1, 2 steps)
* typically padding is not used
- the max pooling is applied likewise to each of the channels in the input layer
- as there there are no parameters to learn, there is nothing for backprop to adjust 

> Convolutions with Pooling:
- typically a layer will have a convolution applied, followed by a pooling activations
- together, these are regarded as a single layer in the ConvNet

> Architecture:
- typical pattern: Conv/Pooling --> Conv/Pooling --> FullyConnected (FC) --> FC --> softmax output
- the activation size tends to slowly decrease from input through output layer

> Why Convolutions?
- significantly reduces number of parameters to train (just the filter size by # of filters)
- Parameter Sharing: filters share parameters across the entire input image 
- Sparsity of Connections: each output value only depends on a small number of input values (ie: it's not fully connected!)


== WEEK 2 - DEEP CONVOLUTIONAL MODELS: CASE STUDIES ==
> Case Studies
1. Le Net 5
- two filter layers, including: filter --> activation --> average pooling
- sigmoid / tanh activation
- height & width decreased over the layers, while the channels increased
- finished with two fully connected layers, with a single y-hat prediction
- about 60k parameters
2. AlexNet
- 4 channel images
- ReLU activation 
- filter / max pool / filter / max pool / filter / filter / filter / max pool / FC1 / FC2 / softmax
- 60m parameters
3. VGG-16
- 3 channel images
- 2 * filter (64 channel) / max pool / 2 * filter (128 channel) / max pool / 3 * filter (256 channel) / max pool / 3 * filter (512 channel) / max pool / 3 * filter (512 channel) / max pool / FC1 / FC2 / softmax
- 138m parameters

> Residual Nets
- very deep networks shoudl in theory train towards a reduction in the loss function, however, increased depth can give rise an upward bend in the loss function, suggesting nets are harder to train
- a proposal was create "Residual Blocks", where additional data can skip some layers and feed into a later linear function, prior to activiation (ie: ReLU)
- this seems to improve training efficacy (loss) with deeper layers -- this is called a "short circuit" connection
- basically, the identity function is easy for the Residual Block to learn
- Using a skip-connection helps the gradient to backpropagate and thus helps you to train deeper networks
- with the skip, we need to ensure that the added layer input to the later layer has teh same dimensions, thus, many network use "same" padding to retain layer width & height dimensions

> Network in Network - 1 x 1 x nc convolution
- these can help shrink the number of channels (or grow them!)
- simply multiplys a value by a 1 x 1 unit, through all layers
- facilitiates for addition of non-linearity (ie: ReLU)

> Inception Network
- what this does is basically stack up multiple convolutions (with the same width & height) - in the channel direction
- this allows use to try multiple ideas, such as different filters, channels, and pooling layers, in one hit
- by using a 1 x 1 convolution, we can use this as a "bottle neck" to reduce the number of computations in getting the desired output ==> of course, you'll end up with far fewer parameters, however, it trains much faster

> Transfer Learning
- what one can do is copy teh weights from a previously trained network, and just train with changes to the softmax output layer
- this is known as freezing the training layers, and framworks such as Keras support this 
- if one has a lot of training data, we could choose just to freeze the early layers, and train on the later layers
- freezing allows us to use the weights trained by others - weights that might be good for identifying basic features - and focus on the final image detection
- at the very least, pre-trained weights are a good beginning for weight initialisation

> Data Augmentation
- mirroring images
- random cropping
- rotation
- shearing
- local warping
- colour shifting (R,G,B channels), according to a sensible distribution, ie: PCA colour augmentation

> Winning on benchmarks
- ensembling (average predictions from multiple models)

> use open source code, as benefits include"
- literature published network architectures
- open source implementations
- pre-trained models, allowing one to focus on fine tuning