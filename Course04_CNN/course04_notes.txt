COURSE 04 of 05:FOUNDATIONS OF CONVOLUTIONAL NEURAL NETWORKS

INDEX:
== WEEK 1 - CONVOLUTIONAL NEURAL NETWORKS ==
== WEEK 2 - DEEP CONVOLUTIONAL MODELS: CASE STUDIES ==
== WEEK 3 - OBJECT DETECTION ==
== WEEK 4 - SPECIAL APPLICATIONS: FACE RECOGNITION & NEURAL STYLE TRANSFER ==

== WEEK 1 - CONVOLUTIONAL NEURAL NETWORKS ==
> Convolutions
- a convolution involves applying a "filter" (or, kernel) to the input data
- we are "convolving" the input data with a filter
- say we had a 6 x 6 input image, and a 3 x 3 filter (1,1,1;0,0,0;-1,-1,-1), we:
* mutiply the filter by a matched segment of the image, get the sum product, report a single output
* for a 6 x 6 image, with a 3 x 3 filter, this produces a 4 x 4 "image" output
* this would appear as conv2d in keras / tf

> Edge detection:
- positive to negative edges refers to light to dark or dark to light edges
- different filters (or, kernels) allow for detection of diffent types of edges (say, horizontal or vertical)
- we can let the NN learn the filter parameters, so that the edge detection can be performed in a way that suits the data being read

> Padding & Applying filters:
- a (n x n ) image convolved with an (f x f) filter produces a ((n - f + 1) x (n - f + 1)) output image
- applying fliters can lead to 
* shrinking output (ie: smaller output image)
* information at corners / extremes of the image are used less often than those towards the center, so information is being lost
- Padding allows us to add a border of zeros around the image, which can help preserve the output size after convolution, and retain all information at the extremes
* so: 
(n x n ) image, with padding (p), convolved with an (f x f) filter produces a ((n + 2p - f + 1) x (n + 2p - f + 1)) output image
- padding is referred to as:
"valid" means "no padding".  
"same" results in padding the input such that the output has the same length as the original input. 

> Stride:
- affects the shift of the filter across the image, ie: stride of 2 will shift the filter by two columns and rows 
* so: 
(n x n ) image, with padding (p), and stride (s), convolved with an (f x f) filter produces a ((n + 2p - f) / s + 1) x ((n + 2p - f) / s + 1) output image
- if there's a partial overlap of the filter with the imgage, then an output value will not be produced

> Multi-channel image convolution:
- 3D filters (H x W x Channel / Depth) is also available for image convolutions
- the number of channels in the filter (depth) must match the number of channels in the image
- as earlier, a (6 x 6 x 3) image convolved with a (3 x 3 x 3) filter will produced a (4 x 4) output, assuming valid padding and stride of 1
* so: 
(n x n x nc) image, with padding (p), and stride (s), convolved with an (f x f x nc) filter produces a ((n + 2p - f) / s + 1) x ((n + 2p - f) / s + 1) output image, where nc = number of channels
- we can apply different filters (kernels defined differently) of same shape to the input image, then stack the outputs!!
* ie: for example, we can have a filter for vertical edge detection, horizontal edge detction, and other types of edge detction...awesome!

> A Layer of a ConvNet:
1. Apply filter to the image
2. Add bias (b) to each cell of the output
3. Apply non-linear activation function (ie: ReLU) to the output
4. Repeat for each filter used, then stack up the outputs to form a Layer of a ConvNet

- The values of the filters become the Weights applied to the input image that the DL NN will learn, as well as the Bias

- Notation:
f[l] = filter size in layer l
p[l] = padidng in layer l
s[l] = stride in layer l
nc[l] = number of filters
input: n[l-1]H x n[l-1]W x nc[l-1] = the height, width and # channels output from the previously layer, becoming input in the current layer, l-1
output: n[l]H x n[l]W x nc[l], where nc[l] is given the number of filters in the current layer
thus (for height & width):
n[l]H = (n[l-1]H + 2p[l]- f[l]) / s[l] + 1
each filter is:
f[l] x f[l] x nc[l-1]
activations:
a[l] = n[l]H x n[l]W x nc[l]
A[l] = m x n[l]H x n[l]W x nc[l], where m reflects # of examples, such as during batch gradient descent
weights:
f[l] x f[l] x nc[l-1] x nc[l], where nc[l] is # filters in current layer
bias:
nc[l], a single bias parameter for each filter in the current layer

> Pooling layer:
- max pooling takes the max of values within a filter; average pooling, the average within a filter
- parameters include:
* filter size (ie: 2 x 2, 3 x 3)
* stride (ie: slide across 1, 2 steps)
* typically padding is not used
- the max pooling is applied likewise to each of the channels in the input layer
- as there there are no parameters to learn, there is nothing for backprop to adjust 

> Convolutions with Pooling:
- typically a layer will have a convolution applied, followed by a pooling activations
- together, these are regarded as a single layer in the ConvNet

> Architecture:
- typical pattern: Conv/Pooling --> Conv/Pooling --> FullyConnected (FC) --> FC --> softmax output
- the activation size tends to slowly decrease from input through output layer

> Why Convolutions?
- significantly reduces number of parameters to train (just the filter size by # of filters)
- Parameter Sharing: filters share parameters across the entire input image 
- Sparsity of Connections: each output value only depends on a small number of input values (ie: it's not fully connected!)


== WEEK 2 - DEEP CONVOLUTIONAL MODELS: CASE STUDIES ==
> Case Studies
1. Le Net 5
- two filter layers, including: filter --> activation --> average pooling
- sigmoid / tanh activation
- height & width decreased over the layers, while the channels increased
- finished with two fully connected layers, with a single y-hat prediction
- about 60k parameters
2. AlexNet
- 4 channel images
- ReLU activation 
- filter / max pool / filter / max pool / filter / filter / filter / max pool / FC1 / FC2 / softmax
- 60m parameters
3. VGG-16
- 3 channel images
- 2 * filter (64 channel) / max pool / 2 * filter (128 channel) / max pool / 3 * filter (256 channel) / max pool / 3 * filter (512 channel) / max pool / 3 * filter (512 channel) / max pool / FC1 / FC2 / softmax
- 138m parameters

> Residual Nets
- very deep networks shoudl in theory train towards a reduction in the loss function, however, increased depth can give rise an upward bend in the loss function, suggesting nets are harder to train
- a proposal was create "Residual Blocks", where additional data can skip some layers and feed into a later linear function, prior to activiation (ie: ReLU)
- this seems to improve training efficacy (loss) with deeper layers -- this is called a "short circuit" connection
- basically, the identity function is easy for the Residual Block to learn
- Using a skip-connection helps the gradient to backpropagate and thus helps you to train deeper networks
- with the skip, we need to ensure that the added layer input to the later layer has teh same dimensions, thus, many network use "same" padding to retain layer width & height dimensions

> Network in Network - 1 x 1 x nc convolution
- these can help shrink the number of channels (or grow them!)
- simply multiplys a value by a 1 x 1 unit, through all layers
- facilitiates for addition of non-linearity (ie: ReLU)

> Inception Network
- what this does is basically stack up multiple convolutions (with the same width & height) - in the channel direction
- this allows use to try multiple ideas, such as different filters, channels, and pooling layers, in one hit
- by using a 1 x 1 convolution, we can use this as a "bottle neck" to reduce the number of computations in getting the desired output ==> of course, you'll end up with far fewer parameters, however, it trains much faster

> Transfer Learning
- what one can do is copy teh weights from a previously trained network, and just train with changes to the softmax output layer
- this is known as freezing the training layers, and framworks such as Keras support this 
- if one has a lot of training data, we could choose just to freeze the early layers, and train on the later layers
- freezing allows us to use the weights trained by others - weights that might be good for identifying basic features - and focus on the final image detection
- at the very least, pre-trained weights are a good beginning for weight initialisation

> Data Augmentation
- mirroring images
- random cropping
- rotation
- shearing
- local warping
- colour shifting (R,G,B channels), according to a sensible distribution, ie: PCA colour augmentation

> Winning on benchmarks
- ensembling (average predictions from multiple models)

> use open source code, as benefits include"
- literature published network architectures
- open source implementations
- pre-trained models, allowing one to focus on fine tuning


== WEEK 3 - OBJECT DETECTION ==
> Object Localisation
- Notation:
* top left of image: (0,0)
* bottom right of image: (1,1)
* center of object detected: bx,by
* width of object detected: bw
* height of object detected: bh

> Network output:
- network output is now: class label (ie: car/motorcycle/etc) and four boundary points (bx, by, bh, bw)
- ie: y = [pc, bx, by, bh, bw, cx1, cx2, cx..., cxn], where: pc = probability there is an object in teh image (1 or 0), locations of image, and class of image (cx to cxn number of classes)

> Sliding Windows detection:
* Originally:
- First: train network on cropped images of labelled data
- Second: scan the larger image with a "sliding window", and pass these cropped images to the neural net to determine whether an object is detected
- problems: very computationally expensive
* With Convolutions:
- treat fully connected layers as convolutions (1x1)
- use a 2x2 max pool to generate an final layer which will match the sliding window
- this allows the output to reflect all sliding windows at once

* intersection over union (IoU): find proportion of bounding boxes that overlaps with object
* non-max suppression: for boundign boxes that overlap, choose one with highest IoU, and rule out the others
* anchor boxes: used to identify discrete objects in overlapping space - object is assigned a grid cell for object's midpoint and an anchor box for the grid cell with the highest IoU

* y outputs: x grid cells * y grid cells * # anchor boxes * vector n (ie: pc, bx, by, bh, bw, c1, c2, c3, cn classes .... repeat for other anchor box)
  

== WEEK 4 - SPECIAL APPLICATIONS: FACE RECOGNITION & NEURAL STYLE TRANSFER ==
> Verification vs recognition
* Verification: 1:1 is this person who we think
* Recognition: 1:K number of persons, so there's a 1 in K % chance of getting this wrong, so the Verification accuracy needs to offset this

> One shot learning
* recognise an image using a single training image of that person's face
* can get around this by constructing a "similarity function", which measures teh difference between the image and a class, then chooses that which falls below a threshold (tau, τ)
* once network is trained, feed in representative images, get output vector, and get the norm of the difference between these vectors - this is called "siamese neural network architecture"
* we train a network to learn parameters such that if x(i) and x(j) are pictures of the same person, the norm of the distance between teh outupt encoding is very small, and if they are different people, the norm of the distance is large
- that is, we consider triplets of an anchor (A, base image), positive (P, matched person), negative (N, non-matched person)
- the distance must be such that: d(A,P) << d(A,N)

* now, a problem is a network might train all the parameters train to produce alike encodings (A, P, N) such that difference functions all result in 0
- to get around this, we add a "margin", ɑ, so: d(A,P) - d(A,N) + ɑ <= 0
- thus, the loss function: L(A,P,N) = max(d(A,P) - d(A,N) + ɑ, 0)

* During training, we want to choose triplets that are difficult to train on, ie: a non-matched face has very similar features to a matched face

* Face verification as supervised learning:
- train pairs of images, with binary classification where the pairs are matched, and 0 where they are not


> Neural style transer
* Extract features from different convnet layers and transferring them to other images
* Layer 1:
- edges
- colours
* Layer 2:
- more complex textures
* Deeper Layers:
- increasing complexity
- neurons become specialised

* generated image is a function of a content image and a style image
* cost function:
- J(G) = alpha . J(content)(C,G) + beta . J(style)(S,G)
- the generated image pixles are randomised, and we then train teh model the network to generate pixel values that minimise the difference between that pixel value and that of the alike style & content images

* style cost function
- measure correlations between layers
- Let: a[l](i,j,k) = activation at (i,j,k).  G[l] is nc[l] x nc[l] style matrix (or, "gram matrix")
(where: l=layer, i=height, j=width, ,k=channel)
