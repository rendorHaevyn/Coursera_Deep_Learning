COURSE 02 of 05: IMPROVING DEEP NEURAL NETWORKS: HYPERPARAMETER TUNING, REGULARIZATION AND OPTIMIZATION

INDEX:
== WEEK 1 - PRACTICAL ASPECTS OF DEEP LEARNING ==
== WEEK 2 - OPTIMIZATION ALGORITHMS ==
== WEEK 3 - HYPERPARAMETER TUNING BATCH NORMALIZATION PROGRAMMING FRAMEWORKS ==

== WEEK 1 - PRACTICAL ASPECTS OF DEEP LEARNING ==
> Applied ML is highly iterative: idea --> code --> experiment (change hyperparameters) --> idea...
> Training / hold-out or OOB or cross-vaidation or development set / test set
- train on training set
- determine which model hyperparameters set works best on the cross-vaidation set
- then evaluate final model on the test set for an unbiased estimate [test: to provide an unbiased estimate of the performance of the final model]
> in teh era of big-data (1million+ rows), the splits: train/CV or development/test can be something like 980k/10k/10k
- it's no longer common practice to go 60%/20%/20%!
> ensure that development and test sets have the same distribution of labelled data as does the training set (ie: K-fold stratified)

> Bias vs variance:
- bias = to underfit
- variance = to overfit
- if HIGH BIAS (underfitting training set well):
-- bigger network, more hidden layers, more hidden nodes
-- run the model for more iterations
-- change optimisation function
-- try different neural network architecture
- if HIGH VARIANCE (overfitting training set):
-- increase data, try regularisation of parameters
-- try different neural network architecture

> L2 & L1 Regularization:
- In logistic regression we can regularize modelled weights by adding a cost for those weights to the cost function
- L2 regularise (euclidean): ƛ / 2m * ||w||^2 
-- also called the "Frobenius norm"
-- The Frobenius norm, sometimes also called the Euclidean norm (a term unfortunately also used for the vector L^2-norm), is matrix norm of an m×n matrix "A" defined as the square root of the sum of the absolute squares of its elements
-- L2 regularisation is also called "weight decay", as the weights are penalised
-- where: ƛ = regularisation parameter and ||w|| is the norm of the weights
-- ||w||^2 = wT * w, or the squared of the euclidean norm
- L1 regularise: ƛ / m * ||w|| 
-- if L1 is used, w will be sparse, which means there will be a lot of zeros in it

> Regularization and overfitting:
- Regularising weights reduces the z function (w[l].a[l-1] + b[l]), forcing it into linear range for activiation functions (like: sigmoid, tanh)
- So essentially, we can end up with a model --> linear model, as regularisation increases

> "Inverted" Dropout Regularization:
- for each feature by example, a random dropout of that node is applied (ie: 20%)
- to adjust for this random dropout, the activation vector is divided by the probability of retaining (ie: 1-0.2=0.8)
-- this ensures that the activation values, a[l-1], are adjusted up to account for the dropout
- dropout works by spreading weights across different features - due to actual "dropout", the algorithm cannot rely one any singular feature to describe the output
- dropout probabibilty can be varied across different layers
-- typically unlikely that dropout would be applied to the input layer or the final output layer

> Other tips:
- Augment Data: modify images / inputs a little to create additional labelled data points
- Early Stopping: stop training of neural network early, reducing overfitting of training set and improving generalisation to test set
-- the downside of early stopping is that the cost function (gradient descent) is not being optimised, and the prevention of overfitting is also being mixed in

> Normalising inputs:
- scale by subtracting mean and divide by sigma squared (variance)
- this needs to be done for all training, dev and test data sets
- we normalise so that the cost function is more symmetric, and gradient descent is more efficient

> Vanishing and Exploding Gradient:
- with deep neural networks, if the layer weights are initialised to:
a. greater than the identify matrix: then the activiation function and gradients can "explode" exponentially in value by the time layer L is reached, ie: A[L] & dA/dL = very high
a. less    than the identify matrix: then the activiation function and gradients can "vanish" exponentially in value by the time layer L is reached,  ie: A[L] & dA/dL = extremely low

> Weight initialisation for deep networks:
- can better solve problem of vanishing or exploding networks
- random assingment of values, and then modify based upon the activation function being usesd, ie:
-- ReLU: W(l) = random * sqrt(2/n(l-1))   ==> "He" initialisation
-- tanh: W(l) = random * sqrt(1/n(l-1))   ==> "Xavier" initialisation

> Gradient Checking:
- Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient (computed using forward propagation).
- Gradient checking is slow, so we don't run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process.
- assists with determing whether backpropagation is being calculated correctly
- steps:
-- reshape all thetas (W, B) into a big concatenated vector
-- do an approximate gradient estimate using ε
-- check whether euclidean norm for ε gradient is similar to calculated gradient

- NOTES: 
-- gradient checking should only be usesd for debugging, and NOT for training
-- if algorithm fails grad check, look at components to identify bug
-- remember regularisation
-- grad check doesnt work with dropout
-- run grad check at random initialisation and after some training to determine if results are similar

** Good weight & bias INITIALISATION will **
- Speed up the convergence of gradient descent
- Increase the odds of gradient descent converging to a lower training (and generalization) error
- The weights  W[l]  should be initialized randomly to break symmetry.
- It is however okay to initialize the biases  b[l]  to zeros. Symmetry is still broken so long as  W[l]  is initialized randomly.
- Poor initialization can lead to vanishing/exploding gradients, which also slows down the optimization algorithm.
- If you train this network longer you will see better results, but initializing with overly large random numbers slows down the optimization.
- Summary notes:
-- Different initializations lead to different results
-- Random initialization is used to break symmetry and make sure different hidden units can learn different things
-- Don't intialize to values that are too large
-- "He" initialization works well for networks with ReLU activations.

** L2 REGULARIZATION Implementation Notes **
- The cost computation:
-- A regularization term is added to the cost
- The backpropagation function:
-- There are extra terms in the gradients with respect to weight matrices
- Weights end up smaller ("weight decay"):
-- Weights are pushed to smaller values.
- Intuition:
-- L2 regularization relies on the assumption that a model with small weights is simpler than a model with large weights. 
-- By penalizing the square values of the weights in the cost function you drive all the weights to smaller values. 
-- It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes.
- Summary notes:
-- Regularisatioan reduces overfitting and drives weights to lower values
-- The value of  λ  is a hyperparameter that you can tune using a dev set.
-- L2 regularization makes your decision boundary smoother. If  λ  is too large, it is also possible to "oversmooth", resulting in a model with high bias.

** DROPOUT Implementation Notes **
- Dropout is a regularization technique that is specific to deep learning. It randomly shuts down some neurons in each iteration.  
- At each iteration, you train a different model that uses only a subset of your neurons. 
- With dropout, your neurons thus become less sensitive to the activation of one other specific neuron, because that other neuron might be shut down at any time.
- Summary notes:
-- Dropout is a regularization technique.
-- You only use dropout during training. Don't use dropout (randomly eliminate nodes) during test time.
-- Apply dropout both during forward and backward propagation.
-- During training time, divide each dropout layer by keep_prob to keep the same expected value for the activations. 
For example, if keep_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. 
Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep_prob is other values than 0.5.


== WEEK 2 - OPTIMIZATION ALGORITHMS ==
> Mini-batch gradient descent
- with big data, training on the entire training set, or batch, can be very slow
- a solution is to train on mini-batches of, say, 1,000 examples, which speeds up the gradient descent
- with mini-batch, the cost function might not decrease smoothly, as we're training on different subset examples each iteration
- "stochastic" gradient descent is where the mini-batch size is 1 (ie: online learning)
-- stochastic gradient descent is very noisy, and heads towards the minimum, however, will never converge
- batch gradient descent (examples=m) --> mini-batch --> stochastic (examples=1)
- choices:
-- small training set (m=2000) - use batch gd
-- typical mini-batch size - 2^6 to 2^9
- before mini-batch, the training data need to be shuffled and then partitioned

Notation: use {t} to denote the batch, t

> Exponentially weighted moving average
Vt = βVt-1 + (1-β)θt  -- ie: a weight by the previous value, plus a proportion of the current observation
- very efficient and simply one-line-of-code to estimate moving averages across multipel variables

Bias Correction:
- early on the estimates can under-estimate the averages, so a "bias correction" can address this, thus implement:
Vt = βVt-1 + (1-β)θt / (1-β^t)  -- as the number of iterations increase, this bias adjustment decreases in effect

> GD Alternative: Gradient descent with Momentum
- Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.
- faster than standard gradient descent
- exponentially weighted moving average of the gradients, which is used to determine gradient descent
- reduces oscillation which would otherwise slow down gradient descent and prevent the use of a larger learning rate
-- ie: the ball is allowed to roll down the gradient, gaining momentum, without spinning out to the side
- typically a β=0.9 is used, so we're averaging over the last 10 iterations gradient
How do you choose  β ?
1. The larger the momentum β is, the smoother the update because the more we take the past gradients into account. But if β is too big, it could also smooth out the updates too much.
2. Common values for β range from 0.8 to 0.999. If you don't feel inclined to tune this, β=0.9 is often a reasonable default.
3. Tuning the optimal β for your model might need trying several values to see what works best in term of reducing the value of the cost function J.

> GD Alternative: RMSprop (Root Mean Square prop)
- the derivatives are squared (at some point?) and later a squared root is applied
- reduces oscillation which would otherwise slow down gradient descent and prevent the use of a larger learning rate
- parameters: β, ε

> GD Alternative: Adam optimisation (Adaptive moment estimation)
- takes Momentum (including bias correction) and RMSprop and uses them together
- hyperparameters: ɑ (tune this!), β (Momentum, ~0.9), β (RMSprop, ~0.999), ε (10^-8)
1. It calculates an exponentially weighted average of past gradients, and stores it in variables ʋ (before bias correction) and ʋ-corrected (with bias correction).
2. It calculates an exponentially weighted average of the squares of the past gradients, and stores it in variables ss (before bias correction) and scorrectedscorrected (with bias correction).
3. It updates parameters in a direction based on combining information from "1" and "2".

> Learning Rate Decay:
- decay the learning rate over time to assist with convergence
- an "epoch" represents a pass of the entire training dataset, when using mini-batches
- we can decay the learning rate according to the number of epochs (full training set passes)
- eg:
-- linear decay:  				ɑ = ɑ0 * 1/(1 + decay_rate * nbr_epochs)
-- exponential decay: 			ɑ = ɑ0 * 0.95^(nbr_epochs)
-- square root epoch decay: 	ɑ = ɑ0 * 1/sqrt(nbr_epochs)  	, where k = constant (say, 1)
-- binary decay:				ɑ = ɑ0 * 0.5^(nbr_epochs)
-- square root mini-batch decay:ɑ = ɑ0 * k/sqrt(t)   			, wherk k = constant and t = number of mini batches

> Local optima
- with high-dimensional spaces
-- we are more likely to encounter saddle points than get stuck in bad local optima
-- plateaus (long, flat regions) can slow down learning 
-- thus algos like Momentum/RMSprop/Adam can help pertubate learning along plateaus and find the next path to descent


== WEEK 3 - HYPERPARAMETER TUNING BATCH NORMALIZATION PROGRAMMING FRAMEWORKS ==
> Hyperparameter tuning, order of importance:
1.1 ɑ learning rate
2.1 β (Momentum gradient descent)
2.2 mini-batch size
2.3 # hidden units
3.1 # layers
3.2 learning rate decay & learning rate decay method
4.1 Adam algorithm - typically dont bother tuning these: β1 (adam first moment), β2 (adam second moment), ε

- Approach
-- try a random values, rather than a grid (list) of values: this will increase the variety of values utilised in the search
-- start with a "coarse" search, then when a good space is found, zoom in with "fine" search
-- for parameters which exist across a large range, log sampling might be a good approach to select evenly over the range, ie: randomly select float (-a:b) then raise to power 10 for hyperparameter
eg: for β hyperparameter, consider range 0.9--0.999, which is like (1-β):{0.1,0.001}, so (1-β) ranges from 10^-1--10^-3, take random value r=-3:-1, raise 10^-r for hyperparameter value
-- train 1 or 2 models in small iterations / steps and zoom-in on successes, or, train a lot of models in parallel

> Batch Normalisation
- first, we have normalised the input features, a[0]
- second, we normalise the linear activations at each layer, z[1]-->z[L-1]
-- note: we normalise the linear activation, not the transform activations (ie: ReLU, sigomid, tanh, etc)
- the z's are normalised, then modified according to parameters ɣ and β
step 1: z[l](norm) 	= (z[l] - mu) / sqrt(variance + epsilon)
step 2: z[l](tilda) = ɣ * z[l](norm) + β
- these parameters, ɣ and β, are learnable hyperparameters





